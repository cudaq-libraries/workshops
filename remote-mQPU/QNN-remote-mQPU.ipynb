{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote-mQPU:\n",
    "\n",
    "<div>\n",
    "<img src=\"./remote-mqpu.png\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "the multi-QPU NVIDIA platform enables multi-QPU distribution whereby each QPU is simulated by a single NVIDIA GPU. To run multi-QPU workloads on different simulator backends, one can use the remote-mqpu platform, which encapsulates simulated QPUs as independent HTTP REST server instances.\n",
    "\n",
    "By default, auto launching daemon services do not support MPI parallelism. Hence, using the nvidia-mgpu backend to simulate each virtual QPU requires manually launching each server instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0,1 mpiexec -np 2 python3 cudaq-qpud --port <QPU 1 TCP/IP port number>\n",
    "CUDA_VISIBLE_DEVICES=2,3 mpiexec -np 2 python3 cudaq-qpud --port <QPU 2 TCP/IP port number>\n",
    "CUDA_VISIBLE_DEVICES=4,5 mpiexec -np 2 python3 cudaq-qpud --port <QPU 3 TCP/IP port number>\n",
    "CUDA_VISIBLE_DEVICES=6,7 mpiexec -np 2 python3 cudaq-qpud --port <QPU 4 TCP/IP port number>\n",
    "```\n",
    "\n",
    "```<QPU n TCP/IP port number>```: The network ports just need to be available, so you can pick random numbers between ~1100 and ~49000.\n",
    "\n",
    "Note: If the port is unavailable, it will report an error saying something like \"failed to bind to port\"\n",
    "\n",
    "Note: When you are done with the servers, you need to manually kill them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "import cudaq\n",
    "from cudaq import spin\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "backend = 'nvidia-mgpu'\n",
    "servers = \"localhost:30001,localhost:30002\"\n",
    "\n",
    "# Set the target to execute on and query the number of QPUs in the system;\n",
    "# The number of QPUs is equal to the number of (auto-)launched server instances.\n",
    "cudaq.set_target(\"remote-mqpu\",\n",
    "                    backend=backend,\n",
    "                    auto_launch=str(servers) if servers.isdigit() else \"\",\n",
    "                    url=\"\" if servers.isdigit() else servers)\n",
    "qpu_count = cudaq.get_target().num_qpus()\n",
    "print(\"Number of virtual QPUs:\", qpu_count)\n",
    "\n",
    "qubit_count = 30\n",
    "sample_count = 2\n",
    "\n",
    "ham = spin.z(0)\n",
    "\n",
    "parameter_count = qubit_count\n",
    "\n",
    "\n",
    "# Below we run a circuit for 500 different input parameters.\n",
    "parameters = np.random.default_rng(13).uniform(low=0,high=1,size=(sample_count,parameter_count))\n",
    "\n",
    "print('Parameter shape: ', parameters.shape)\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel_rx(theta:list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "\n",
    "    for i in range(qubit_count):\n",
    "        rx(theta[i], qubits)\n",
    "\n",
    "# Multi-GPU\n",
    "\n",
    "# We split our parameters into 2 arrays \n",
    "xi = np.split(parameters,2)\n",
    "\n",
    "print('We have', parameters.shape[0],\n",
    "      'parameters which we would like to execute')\n",
    "\n",
    "print('We split this into', len(xi), 'batches of', xi[0].shape[0], ',',\n",
    "      xi[1].shape[0])\n",
    "\n",
    "\n",
    "print('Shape after splitting', xi[0].shape)\n",
    "asyncresults = []\n",
    "\n",
    "\n",
    "for i in range(len(xi)):\n",
    "    for j in range(xi[i].shape[0]):\n",
    "        asyncresults.append(\n",
    "            cudaq.observe_async(kernel_rx, ham, xi[i][j, :], qpu_id=i))\n",
    "\n",
    "\n",
    "print('Energies from multi-GPUs')\n",
    "for result in asyncresults:\n",
    "    observe_result = result.get()\n",
    "    got_expectation = observe_result.expectation()\n",
    "    print(got_expectation)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute this job, use:\n",
    "\n",
    "``` python\n",
    "CUDA_VISIBLE_DEVICES=0,1 mpiexec -np 2 cudaq-qpud --port 30001 &\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=2,3 mpiexec -np 2 cudaq-qpud --port 30002 &\n",
    "\n",
    "python3 observe-qml-remote-mqpu.py\n",
    "\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "``` python\n",
    "\n",
    "Number of virtual QPUs: 2\n",
    "Parameter shape:  (2, 30)\n",
    "We have 2 parameters which we would like to execute\n",
    "We split this into 2 batches of 1 , 1\n",
    "Shape after splitting (1, 30)\n",
    "Energies from multi-GPUs\n",
    "-0.17012869483974752\n",
    "-0.9906678983711331\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "cudaq.set_target(\"remote-mqpu\",\n",
    "                    backend=backend,\n",
    "                    auto_launch=str(servers) if servers.isdigit() else \"\",\n",
    "                    url=\"\" if servers.isdigit() else servers)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "If the use case is simple enough, then the CUDA-Q runtime can auto-launch the server for you. For example, if servers is '2', then auto_launch='2' and url=''. The CUDA-Q runtime will interpret that to mean that it should auto-launch 2 servers and it will auto-pick the ports, so the user doesn't need to specify the URL.\n",
    "\n",
    "But if servers is a comma-separated list of servers (like 'localhost:30001,localhost:30002' ), then auto_launch='' and url='localhost:30001,localhost:30002' . That means the CUDA-Q runtime WON'T auto launch the servers and will use the provided url to connect to when submitting programs to the backend."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
