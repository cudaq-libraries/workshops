{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Multi-QPU (nvidia-mqpu)\n",
    "\n",
    "The `nvidia-mqpu` target is useful for distributing separate quantum circuits to individual GPUs on a single host machine. \n",
    "\n",
    "![img](./circuit-mqpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with `sample` algorithmic primitives\n",
    "\n",
    "#### Quantum Restricted Boltzmann Machine (Q-RBM)\n",
    "\n",
    "![img](./RBM.png)\n",
    "\n",
    "![img](./gibbs-dist.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of QPUs: 5\n",
      "{ 10110111:9 00010111:26 11100111:8 00000111:3 10011111:106 10111011:3 11011111:13 10011011:62 01011101:150 11110111:597 10111100:913 10111010:20 10110001:95 11101110:55 11101111:15 10110011:5 01111111:906 00111110:44 10110110:29 11111111:935 01101110:23 11001110:2 00111111:10 00000011:4 00011110:157 01000000:1 01101010:18 01001001:2 00010110:91 11011011:11 11111011:526 01010101:76 10100111:2 01101111:6 11110011:321 11011101:374 00101001:25 10101111:3 11100110:25 01000100:1 10111001:139 11011001:202 01001000:2 00001111:5 10100101:48 10110000:329 01011111:2 10111111:13 01000001:1 10111101:242 10001111:45 10111000:561 10011010:247 01101011:8 10011110:412 00011111:34 10110010:17 10110100:541 01001100:3 10111110:40 11001100:30 11001101:8 00101011:1 00111100:869 00111101:260 10110101:141 00101111:1 00101101:39 01010111:4 10101101:84 }\n",
      "\n",
      "{ 10110111:3 00010111:26 11100111:4 00000111:2 10011111:128 10111011:3 11011111:16 10011011:68 01011101:175 11110111:565 00001011:3 11011011:14 00010110:92 10111100:959 10111010:19 10110001:76 11101110:46 11101111:19 10110011:5 01111111:872 11100110:33 01000100:2 10101111:3 10111001:154 00111110:31 10110110:30 11111111:882 01101110:16 00111111:7 11001110:1 01001000:3 10110000:312 11001111:1 10111111:5 01011111:9 11111011:556 01010101:102 10100111:5 10100101:60 00001111:7 01101111:2 00011110:156 01000000:1 11011001:193 11110011:364 01101010:13 10011010:222 10001111:40 10111000:517 10111101:233 01000101:3 01101011:5 01000110:1 10011110:432 00011111:48 10110010:8 10110100:532 01001100:2 10111110:52 11001100:20 11001101:6 00111100:878 00111101:265 10110101:144 00101111:3 00101101:24 01010111:6 10101101:86 00101001:22 11011101:408 }\n",
      "\n",
      "{ 10110111:6 00010111:24 11100111:9 00000111:9 10011111:106 10111011:5 11011111:19 10011011:72 01011101:136 11110111:537 00001011:6 01001111:1 10110001:83 10111100:926 10111010:27 11101110:49 11101111:11 10110011:3 01111111:924 11100110:41 01000100:4 10101111:5 10111001:151 00111110:36 10110110:27 11111111:928 01101110:20 11001110:3 00111111:5 00000011:1 00010110:105 11011011:15 01001000:4 10111111:7 01011111:5 11111011:544 01010101:70 10100111:5 10100101:62 00001111:5 01101111:8 00011110:147 01000000:1 11011001:220 11110011:322 11011101:382 00101001:19 00101111:1 10110101:163 10110000:307 11001111:2 01101010:10 10001111:48 10111000:532 10111101:222 10011010:271 01101011:3 10011110:456 00011111:31 10110010:18 10110100:514 01001100:3 10111110:40 11001100:24 11001101:10 00101011:1 00111100:886 00111101:258 00101101:24 01010111:2 10101101:79 }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "\n",
    "cudaq.set_target(\"nvidia-mqpu\")\n",
    "\n",
    "target = cudaq.get_target()\n",
    "qpu_count = target.num_qpus()\n",
    "print(\"Number of QPUs:\", qpu_count)\n",
    "\n",
    "@cudaq.kernel\n",
    "def qrbm(v_nodes:int, h_nodes:int, ancilla:int, theta: list[float], coupling: list[float]):\n",
    "\n",
    "    qubits_num=v_nodes+h_nodes+ancilla\n",
    "    qubits=cudaq.qvector(qubits_num)\n",
    "\n",
    "    # Encode the node parameters\n",
    "    for i in range(v_nodes+h_nodes):\n",
    "        ry(theta[i],qubits[i])\n",
    "\n",
    "    # Encode the coupling between nodes\n",
    "    a_target=v_nodes+h_nodes\n",
    "    count=0\n",
    "    for v in range(v_nodes):\n",
    "        for h in range(v_nodes,v_nodes+h_nodes):\n",
    "            ry.ctrl(coupling[count],qubits[v],qubits[h],qubits[a_target])\n",
    "            x(qubits[v])\n",
    "            ry.ctrl(coupling[count+1],qubits[v],qubits[h],qubits[a_target])\n",
    "            x(qubits[v])\n",
    "            x(qubits[h])\n",
    "            ry.ctrl(coupling[count+1],qubits[v],qubits[h],qubits[a_target])\n",
    "            x(qubits[v])\n",
    "            ry.ctrl(coupling[count],qubits[v],qubits[h],qubits[a_target])\n",
    "            x(qubits[v])\n",
    "            x(qubits[h])\n",
    "\n",
    "            count+=2\n",
    "            a_target+=1\n",
    "\n",
    "    mz(qubits)    \n",
    "    \n",
    "v_nodes=2\n",
    "h_nodes=2\n",
    "ancilla=4\n",
    "\n",
    "# Initialize the parameters for the RBM\n",
    "theta=[2.0482, 1.4329, 2.1774, 2.7122]\n",
    "coupling=[1.8256, 3.1415, 1.8257, 3.1415, 3.1415, 0.4152, 3.1415, 0.9654]\n",
    "\n",
    "count_futures = []\n",
    "\n",
    "for qpu in range(3):\n",
    "    count_futures.append(cudaq.sample_async(qrbm,v_nodes, h_nodes, ancilla, theta, coupling, shots_count=10000,qpu_id=qpu))\n",
    "\n",
    "for counts in count_futures:\n",
    "    print(counts.get())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with `observe` algorithmic primitives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of QPUs: 5\n",
      "Parameter shape:  (500, 10)\n",
      "We have 500 parameters which we would like to execute\n",
      "We split this into 4 batches of 125 , 125 , 125 , 125\n",
      "Shape after splitting (125, 10)\n",
      "Energies from multi-GPUs\n",
      "0.6487942264394656\n",
      "0.9592965035114541\n",
      "0.9693367711158675\n",
      "0.9752157540898212\n",
      "0.7901937174208915\n",
      "0.5851651225539602\n",
      "0.9135656129518316\n",
      "0.9490337029063387\n",
      "0.8509625392277542\n",
      "0.7454635553957428\n",
      "0.9305526875050824\n",
      "0.959998285031342\n",
      "0.8932189110754846\n",
      "0.9155879814111777\n",
      "0.9597179527710608\n",
      "0.9935125945096381\n",
      "0.8646527734148779\n",
      "0.6439004804322485\n",
      "0.5830942872137947\n",
      "0.6834044937811433\n",
      "0.9637192885208135\n",
      "0.9989258683207686\n",
      "0.8779315873648825\n",
      "0.8767961513094958\n",
      "0.6468637421465269\n",
      "0.9239566569275575\n",
      "0.7861546173171964\n",
      "0.9656727214945597\n",
      "0.9778974511307598\n",
      "0.5407711059133851\n",
      "0.8048163873900496\n",
      "0.9950455390681953\n",
      "0.8951334235851304\n",
      "0.6489412249260302\n",
      "0.9690827240813413\n",
      "0.9999714740669639\n",
      "0.9928629044231196\n",
      "0.9969285794959803\n",
      "0.7175125779993621\n",
      "0.7025735612926575\n",
      "0.5680724042480292\n",
      "0.9903866425210517\n",
      "0.6651906491041232\n",
      "0.9980324869943888\n",
      "0.8856357601561018\n",
      "0.9298439978863543\n",
      "0.8435624352365433\n",
      "0.8102839913923287\n",
      "0.8620627472269234\n",
      "0.6734933807970559\n",
      "0.6514276554801706\n",
      "0.9450732500389407\n",
      "0.9445913743641696\n",
      "0.7410538514246776\n",
      "0.9959936681411327\n",
      "0.9379315528106338\n",
      "0.67803198507302\n",
      "0.98820892643508\n",
      "0.7942752866292725\n",
      "0.9896384289925145\n",
      "0.7096232509350623\n",
      "0.9191151626303087\n",
      "0.9870728907750523\n",
      "0.7775735499677541\n",
      "0.9512393391128335\n",
      "0.9481713521125978\n",
      "0.7969676707230449\n",
      "0.7368480918222342\n",
      "0.9999904072374617\n",
      "0.9459687264236768\n",
      "0.6262218981178049\n",
      "0.9997222119632673\n",
      "0.9157649330804674\n",
      "0.9366965295777336\n",
      "0.6598650054667179\n",
      "0.6333745647076595\n",
      "0.798244757707621\n",
      "0.549746973760264\n",
      "0.6952470611135857\n",
      "0.9944006913732788\n",
      "0.5910004456701108\n",
      "0.6777291759950418\n",
      "0.8996138496611906\n",
      "0.9620968192963089\n",
      "0.8639789799073818\n",
      "0.8531844788592499\n",
      "0.9411778199718703\n",
      "0.9905203159813761\n",
      "0.8583941486762248\n",
      "0.9749427205409738\n",
      "0.8418263843294544\n",
      "0.7612464518372744\n",
      "0.9172270299312774\n",
      "0.9184551668005297\n",
      "0.8229781488478195\n",
      "0.5827235905273491\n",
      "0.8450769308343276\n",
      "0.9831757198082474\n",
      "0.6900317800026223\n",
      "0.7555967500024448\n",
      "0.7472942097790873\n",
      "0.9810357866661704\n",
      "0.7895885725041059\n",
      "0.9942100527090354\n",
      "0.9424170706217149\n",
      "0.9969274495325485\n",
      "0.6474576662878773\n",
      "0.9756036338531581\n",
      "0.662052082558935\n",
      "0.9388299582463431\n",
      "0.9915084683371607\n",
      "0.9927929990095288\n",
      "0.997611046754328\n",
      "0.9763060928496445\n",
      "0.8718288706713359\n",
      "0.8011613724476971\n",
      "0.8491688357231381\n",
      "0.5642093269548978\n",
      "0.8480348636021487\n",
      "0.8451309976740777\n",
      "0.8817139173419246\n",
      "0.9356148835403805\n",
      "0.6751249992152214\n",
      "0.9636768169475555\n",
      "0.996026467587596\n",
      "0.8172216583127087\n",
      "0.9242779720552622\n",
      "0.992415080899971\n",
      "0.9997818854604397\n",
      "0.999500104110568\n",
      "0.9891505041445888\n",
      "0.9718485484498948\n",
      "0.9195068771422653\n",
      "0.9563498075939745\n",
      "0.8067293901912421\n",
      "0.9978507046646095\n",
      "0.7620568002331596\n",
      "0.975178509072704\n",
      "0.6862119674128975\n",
      "0.7367233486743315\n",
      "0.8168233483528159\n",
      "0.9680601347529996\n",
      "0.7752730333306521\n",
      "0.7119540036518089\n",
      "0.9698567270671598\n",
      "0.8992503410835511\n",
      "0.5521711372263636\n",
      "0.6625159732932839\n",
      "0.667943992257815\n",
      "0.8671074175527126\n",
      "0.9658121467694838\n",
      "0.8460316797868164\n",
      "0.7917302809331828\n",
      "0.8359903392039475\n",
      "0.6146239512440135\n",
      "0.9672946053534351\n",
      "0.8798949449767842\n",
      "0.9962833289292587\n",
      "0.6953662797344358\n",
      "0.7670638026736352\n",
      "0.5815110883565405\n",
      "0.7887811348511521\n",
      "0.9999597797167861\n",
      "0.6220406607164171\n",
      "0.9958927854133109\n",
      "0.9430795660608947\n",
      "0.8502932020269387\n",
      "0.6190459660486846\n",
      "0.9908185301753706\n",
      "0.9842658349267306\n",
      "0.9999833691967563\n",
      "0.9962663534139345\n",
      "0.8367894824640291\n",
      "0.988797507332607\n",
      "0.8351356816196466\n",
      "0.6659331781113645\n",
      "0.8500645738891973\n",
      "0.9097130921866605\n",
      "0.6956389482374562\n",
      "0.9235825515512992\n",
      "0.7855533710480468\n",
      "0.8749137846817964\n",
      "0.8836811701944469\n",
      "0.760166866117142\n",
      "0.912338485926486\n",
      "0.5856183833327853\n",
      "0.8155914305798391\n",
      "0.9995569716998318\n",
      "0.7502421744329225\n",
      "0.5592605810747873\n",
      "0.6490271112647525\n",
      "0.9797876438379446\n",
      "0.8279268352996956\n",
      "0.9288689327478227\n",
      "0.8179094437793463\n",
      "0.8256370716763842\n",
      "0.9990802583222255\n",
      "0.58448986721499\n",
      "0.5805235277188994\n",
      "0.9299037157959484\n",
      "0.9234433546328918\n",
      "0.9445758330903166\n",
      "0.9994622190280458\n",
      "0.9975886383238332\n",
      "0.6855675914381198\n",
      "0.5849645675957038\n",
      "0.7111488249927992\n",
      "0.8946639230775484\n",
      "0.6520338879038479\n",
      "0.9607055576917296\n",
      "0.6437245240082724\n",
      "0.8456398361609917\n",
      "0.8226210433503192\n",
      "0.8804537090847857\n",
      "0.8311100408351078\n",
      "0.9961359436998698\n",
      "0.9953022324162004\n",
      "0.7174835394328581\n",
      "0.6516422879984635\n",
      "0.6353596505960367\n",
      "0.8199681231455359\n",
      "0.9883049859233421\n",
      "0.9991287105031881\n",
      "0.6504320736744649\n",
      "0.9067875870061284\n",
      "0.6775980635227752\n",
      "0.6291032816031426\n",
      "0.9300538119781383\n",
      "0.9507500460006171\n",
      "0.9949338758705613\n",
      "0.9969964463557663\n",
      "0.8864811581471567\n",
      "0.7603585002840627\n",
      "0.8238794773974739\n",
      "0.6802717891474768\n",
      "0.7727122864726682\n",
      "0.8376909457444476\n",
      "0.9382152483270617\n",
      "0.9874504822297079\n",
      "0.907749665564222\n",
      "0.6344631072133677\n",
      "0.9993015073074156\n",
      "0.5866644295866873\n",
      "0.7365504917555502\n",
      "0.9959946887911828\n",
      "0.995209013720415\n",
      "0.8675439101888426\n",
      "0.654780174321971\n",
      "0.9919867118076673\n",
      "0.990574949321744\n",
      "0.9646792262756979\n",
      "0.7184877620229602\n",
      "0.7145231395702963\n",
      "0.9999580706181019\n",
      "0.77194794448381\n",
      "0.9609913006022599\n",
      "0.7568942466077536\n",
      "0.9967385959252337\n",
      "0.9762761832639033\n",
      "0.6704309865737492\n",
      "0.9781446180779649\n",
      "0.9612749139674941\n",
      "0.6790583671199837\n",
      "0.9166751757257767\n",
      "0.9975954145797447\n",
      "0.9697410310024538\n",
      "0.5653501255647092\n",
      "0.7943708711664399\n",
      "0.9147792650396276\n",
      "0.9901051705258346\n",
      "0.6382999808072336\n",
      "0.632371257328904\n",
      "0.6185745954292451\n",
      "0.5532687357384272\n",
      "0.720191713357716\n",
      "0.7682386225623654\n",
      "0.9473636186824621\n",
      "0.7268659555563923\n",
      "0.8599137792593561\n",
      "0.6506449974208852\n",
      "0.8570084932773598\n",
      "0.9228291707032871\n",
      "0.7566103482204516\n",
      "0.6034514530055762\n",
      "0.9811043129878678\n",
      "0.9415869936494957\n",
      "0.9186414038243893\n",
      "0.9649608360749371\n",
      "0.9665325971924807\n",
      "0.9203518231496954\n",
      "0.564514375564219\n",
      "0.7193582441490112\n",
      "0.9991504117341258\n",
      "0.9994737103504192\n",
      "0.7293102290963478\n",
      "0.8785294904607741\n",
      "0.7093249282427477\n",
      "0.9156436196437104\n",
      "0.9829196818844482\n",
      "0.9948821884636099\n",
      "0.9816158801728038\n",
      "0.5526460101460298\n",
      "0.9930290146021366\n",
      "0.7543677471032643\n",
      "0.5718483885145991\n",
      "0.5975681630637464\n",
      "0.8322082027622579\n",
      "0.9050592568032716\n",
      "0.8877619461436584\n",
      "0.9556935319655628\n",
      "0.9860130473251706\n",
      "0.8953869668591545\n",
      "0.9714094408906706\n",
      "0.99807798768877\n",
      "0.8731068843180849\n",
      "0.7410659740304176\n",
      "0.6545587237695099\n",
      "0.9897665304902763\n",
      "0.8034380782302851\n",
      "0.8338088422874245\n",
      "0.8230899825684892\n",
      "0.7537821834375826\n",
      "0.8854181198819051\n",
      "0.7495090829796331\n",
      "0.6418903500779449\n",
      "0.544059392097743\n",
      "0.8549876661379643\n",
      "0.6237629206259031\n",
      "0.8912445162823207\n",
      "0.8826714275154536\n",
      "1.0000000340430317\n",
      "0.9334648174375371\n",
      "0.77510798268224\n",
      "0.9871603472995728\n",
      "0.7713958775540757\n",
      "0.945363869362727\n",
      "0.7994860605661366\n",
      "0.6545273128090385\n",
      "0.8486155895923075\n",
      "0.9287592544988894\n",
      "0.6670696872650035\n",
      "0.9263680644819665\n",
      "0.8988148678126964\n",
      "0.7427624922239962\n",
      "0.629857089350079\n",
      "0.5547218200905749\n",
      "0.9986330710539685\n",
      "0.7181519937688332\n",
      "0.9969775963227453\n",
      "0.8664748153003469\n",
      "0.9541469340798784\n",
      "0.8809930291999971\n",
      "0.9721344241518503\n",
      "0.5576646792446844\n",
      "0.9779519038804623\n",
      "0.680015583589464\n",
      "0.6854694472157093\n",
      "0.5938213895640548\n",
      "0.9787190077680769\n",
      "0.8023893376536065\n",
      "0.9424785818057511\n",
      "0.999177690888263\n",
      "0.9699524310046458\n",
      "0.745513410650079\n",
      "0.8917067362893591\n",
      "0.9958076982688331\n",
      "0.9364434715159512\n",
      "0.6866143477612064\n",
      "0.7675263829975164\n",
      "0.9937366206072068\n",
      "0.9798416672515307\n",
      "0.9552202127718551\n",
      "0.9768110378739648\n",
      "0.7419594704769915\n",
      "0.6336188862568548\n",
      "0.9916940695038898\n",
      "0.7456281134124342\n",
      "0.6595878665439626\n",
      "0.9900304257918213\n",
      "0.9468634372381279\n",
      "0.9959159760750886\n",
      "0.9759791665983262\n",
      "0.8389625703785\n",
      "0.9994076572815712\n",
      "0.7357916701390232\n",
      "0.9511499465983404\n",
      "0.9965381220237717\n",
      "0.9629568464201481\n",
      "0.7530408065807344\n",
      "0.6970612472588141\n",
      "0.9692365113027519\n",
      "0.8239247396263346\n",
      "0.8407887224233213\n",
      "0.9801285103776156\n",
      "0.9333803735066872\n",
      "0.9708866277177808\n",
      "0.7270578552327308\n",
      "0.7174341487220575\n",
      "0.8752111845931294\n",
      "0.9975188840733977\n",
      "0.9998798137641337\n",
      "0.992922747279709\n",
      "0.7695598776137441\n",
      "0.9995277621348859\n",
      "0.8235922383321834\n",
      "0.9745589487836039\n",
      "0.6715336721359645\n",
      "0.7814065640945049\n",
      "0.7478992453827731\n",
      "0.8887289681212398\n",
      "0.7435549147558673\n",
      "0.6047778299459582\n",
      "0.8733141415842098\n",
      "0.9258462814047146\n",
      "0.9863353228459859\n",
      "0.7933384232605345\n",
      "0.9847888397387666\n",
      "0.9156339491320666\n",
      "0.9552453809744829\n",
      "0.9721375488530528\n",
      "0.5915961441188062\n",
      "0.967746362770124\n",
      "0.8162366398201069\n",
      "0.5920667276064078\n",
      "0.7890902741466395\n",
      "0.9001919018457576\n",
      "0.958315687239407\n",
      "0.9945854311896338\n",
      "0.6174227445809974\n",
      "0.9921228542464282\n",
      "0.5901473493442164\n",
      "0.9673596064104153\n",
      "0.6994891108132962\n",
      "0.9949887125976352\n",
      "0.8062470526429244\n",
      "0.6758537835544438\n",
      "0.9929480907142418\n",
      "0.7671532101369245\n",
      "0.9999600118498052\n",
      "0.9343429409616063\n",
      "0.9996439968964357\n",
      "0.6791922110707802\n",
      "0.8710949584446492\n",
      "0.5890614458627359\n",
      "0.9016128328218734\n",
      "0.97505280113956\n",
      "0.7613060853847661\n",
      "0.9950602751888695\n",
      "0.955685443977156\n",
      "0.9344739743440861\n",
      "0.9735922557997484\n",
      "0.7919064283116078\n",
      "0.9699930145139274\n",
      "0.5459034077739824\n",
      "0.996880453094834\n",
      "0.9204714301999606\n",
      "0.8012855962046215\n",
      "0.634821632678631\n",
      "0.5819379673862134\n",
      "0.9600230610372069\n",
      "0.9358690125552208\n",
      "0.9854554042025904\n",
      "0.5631184406460864\n",
      "0.9246840986288009\n",
      "0.9663912014533147\n",
      "0.9828109917593129\n",
      "0.7468321007561438\n",
      "0.7840382486396723\n",
      "0.9909579795425488\n",
      "0.9999951751165835\n",
      "0.8268452259566037\n",
      "0.6604572075358301\n",
      "0.9770750066899447\n",
      "0.9414427054764057\n",
      "0.9722422246620326\n",
      "0.8504519303894463\n",
      "0.9779822246390389\n",
      "0.9920041511078297\n",
      "0.5944094851046169\n",
      "0.9756424948564255\n",
      "0.9858521260829355\n",
      "0.9447763483913934\n",
      "0.9788212694042095\n",
      "0.6692344408988509\n",
      "0.5948370394555027\n",
      "0.991072088242882\n",
      "0.5495951693145121\n",
      "0.995682511481509\n",
      "0.5625386984950692\n",
      "0.9602426767776224\n",
      "0.9568824378526302\n",
      "0.9699569361093319\n",
      "0.9636091874048236\n",
      "0.5820060633445548\n",
      "0.6080541478715304\n",
      "0.5583649582799195\n",
      "0.8789762857502635\n",
      "0.670073559569129\n",
      "0.9783447613348747\n",
      "0.6871556852810703\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "from cudaq import spin\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "cudaq.set_target(\"nvidia-mqpu\")\n",
    "target = cudaq.get_target()\n",
    "qpu_count = target.num_qpus()\n",
    "print(\"Number of QPUs:\", qpu_count)\n",
    "\n",
    "qubit_count = 10\n",
    "sample_count = 500\n",
    "\n",
    "ham = spin.z(0)\n",
    "\n",
    "parameter_count = qubit_count\n",
    "\n",
    "# Below we run a circuit for 500 different input parameters.\n",
    "parameters = np.random.default_rng(13).uniform(low=0,high=1,size=(sample_count,parameter_count))\n",
    "\n",
    "print('Parameter shape: ', parameters.shape)\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel_rx(theta:list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "\n",
    "    for i in range(qubit_count):\n",
    "        rx(theta[i], qubits[i])\n",
    "\n",
    "# Multi-GPU\n",
    "\n",
    "# We split our parameters into 4 arrays since we have 4 GPUs available.\n",
    "xi = np.split(parameters,4)\n",
    "\n",
    "print('We have', parameters.shape[0],\n",
    "      'parameters which we would like to execute')\n",
    "\n",
    "print('We split this into', len(xi), 'batches of', xi[0].shape[0], ',',\n",
    "      xi[1].shape[0], ',', xi[2].shape[0], ',', xi[3].shape[0])\n",
    "\n",
    "print('Shape after splitting', xi[0].shape)\n",
    "asyncresults = []\n",
    "\n",
    "\n",
    "for i in range(len(xi)):\n",
    "    for j in range(xi[i].shape[0]):\n",
    "        qpu_id = i * 4 // len(xi)\n",
    "        asyncresults.append(\n",
    "            cudaq.observe_async(kernel_rx, ham, xi[i][j, :], qpu_id=qpu_id))\n",
    "\n",
    "print('Energies from multi-GPUs')\n",
    "for result in asyncresults:\n",
    "    observe_result = result.get()\n",
    "    got_expectation = observe_result.expectation()\n",
    "    print(got_expectation)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch the Spin Hamiltonian terms:\n",
    "\n",
    "![img](./ham-batch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time (s) for single-GPU:  2.3008233257569373\n",
      "Elapsed time (s) for multi-GPU:  5.22126400610432\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "from cudaq import spin\n",
    "\n",
    "import timeit\n",
    "\n",
    "cudaq.set_target(\"nvidia-mqpu\")\n",
    "\n",
    "#cudaq.mpi.initialize()\n",
    "\n",
    "qubit_count = 22\n",
    "term_count = 100000\n",
    "\n",
    "@cudaq.kernel\n",
    "def batch_ham():\n",
    "    qubits=cudaq.qvector(qubit_count)\n",
    "    h(qubits[0])\n",
    "    for i in range(1, qubit_count):\n",
    "        x.ctrl(qubits[0], qubits[i])\n",
    "\n",
    "# We create a random Hamiltonian\n",
    "hamiltonian = cudaq.SpinOperator.random(qubit_count, term_count)\n",
    "\n",
    "# The observe calls allows us to calculate the expectation value of the Hamiltonian with respect to a specified kernel.\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "# Single node, single GPU.\n",
    "result = cudaq.observe(batch_ham, hamiltonian).expectation()\n",
    "end_time = timeit.default_timer()\n",
    "print('Elapsed time (s) for single-GPU: ', end_time-start_time)\n",
    "\n",
    "# If we have multiple GPUs/ QPUs available, we can parallelize the workflow with the addition of an argument in the observe call.\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "# Single node, multi-GPU.\n",
    "result = cudaq.observe(batch_ham, hamiltonian, execution=cudaq.parallel.thread).expectation()\n",
    "end_time = timeit.default_timer()\n",
    "print('Elapsed time (s) for multi-GPU: ', end_time-start_time)\n",
    "\n",
    "\n",
    "# Multi-node, multi-GPU. (if included use mpirun -np n filename.py)\n",
    "#result = cudaq.observe(batch_ham, hamiltonian, execution=cudaq.parallel.mpi).expectation()\n",
    "\n",
    "#cudaq.mpi.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multi-GPU (nvidia-mgpu)\n",
    "\n",
    "The `nvidia-mgpu` backend is useful for running a large single quantum circuit spread across multiple GPUs.\n",
    "- A $n$ qubit quantum state has $2^n$ complex amplitudes, each of which require 8 bytes of memory to store. Hence the total memory required to store a n qubit quantum state is $8$ bytes $\\times 2^n$. For $n=30$ qubits, this is roughly $8$ GB but for $n=40$, this exponentially increases to $8700$ GB.\n",
    "\n",
    "#### Example: GHZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# mpirun -np 4 python <fname> --target nvidia-mgpu\n",
    "\n",
    "import cudaq\n",
    "\n",
    "cudaq.mpi.initialize()\n",
    "\n",
    "qubit_count = 30\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel(qubit_num: int):\n",
    "    # Allocate our qubits.\n",
    "    qvector = cudaq.qvector(qubit_num)\n",
    "    # Place the first qubit in the superposition state.\n",
    "    h(qvector[0])\n",
    "    # Loop through the allocated qubits and apply controlled-X,\n",
    "    # or CNOT, operations between them.\n",
    "    for qubit in range(qubit_num - 1):\n",
    "        x.ctrl(qvector[qubit], qvector[qubit + 1])\n",
    "    # Measure the qubits.\n",
    "    mz(qvector)\n",
    "\n",
    "#print(\"Preparing GHZ state for\", qubit_count, \"qubits.\")\n",
    "counts = cudaq.sample(kernel, qubit_count)\n",
    "\n",
    "if cudaq.mpi.rank() == 0:\n",
    "    print(counts)\n",
    "\n",
    "cudaq.mpi.finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python ghz.py --target nvidia-mgpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
