{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-QPU (nvidia-mqpu)\n",
    "\n",
    "The `nvidia-mqpu` target is useful for distributing separate quantum circuits to individual GPUs on a single host machine. \n",
    "![mqpu](./circuit-mqpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with `sample` algorithmic primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of QPUs: 5\n",
      "{ 000:10000 }\n",
      "\n",
      "{ 000:9925 010:20 100:23 001:32 }\n",
      "\n",
      "{ 000:9697 010:94 111:1 100:108 110:1 001:97 101:1 011:1 }\n",
      "\n",
      "{ 000:9354 010:211 100:223 110:8 001:201 011:3 }\n",
      "\n",
      "{ 000:8807 010:386 111:1 100:396 110:9 001:367 101:21 011:13 }\n",
      "\n",
      "{ 000:8232 010:538 111:1 100:573 110:37 001:552 101:37 011:30 }\n",
      "\n",
      "{ 000:7621 010:767 111:7 100:691 110:78 001:703 101:57 011:76 }\n",
      "\n",
      "{ 000:6852 010:918 111:9 100:945 110:123 001:904 101:120 011:129 }\n",
      "\n",
      "{ 000:6084 010:1101 111:38 100:1093 110:204 001:1075 101:207 011:198 }\n",
      "\n",
      "{ 000:5262 010:1262 111:71 100:1230 110:305 001:1265 101:286 011:319 }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "\n",
    "cudaq.set_target(\"nvidia-mqpu\")\n",
    "\n",
    "target = cudaq.get_target()\n",
    "num_qpus = target.num_qpus()\n",
    "print(\"Number of QPUs:\", num_qpus)\n",
    "\n",
    "\n",
    "@cudaq.kernel\n",
    "def test(i:int):\n",
    "    num_qubits = 3\n",
    "    qubits = cudaq.qvector(num_qubits)\n",
    "    ry(0.1 * i, qubits)\n",
    "    mz(qubits)\n",
    "\n",
    "\n",
    "count_futures = []\n",
    "for i in range(10):\n",
    "    qpu_id = i % num_qpus\n",
    "    count_futures.append(cudaq.sample_async(test, i, shots_count=10000, qpu_id=qpu_id))\n",
    "\n",
    "for counts in count_futures:\n",
    "    print(counts.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with `observe` algorithmic primitives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of QPUs: 5\n",
      "We have 10 parameters which we would like to execute\n",
      "We split this into 5 batches of 100, 100, 100, 100, 100\n",
      "Shape after splitting (100, 10)\n",
      "Energies from multi-GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import cudaq\n",
    "from cudaq import spin\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "cudaq.set_target(\"nvidia-mqpu\")\n",
    "target = cudaq.get_target()\n",
    "num_qpus = target.num_qpus()\n",
    "print(\"Number of QPUs:\", num_qpus)\n",
    "\n",
    "num_qubits = 10\n",
    "sample_count = 500\n",
    "\n",
    "ham = spin.z(0)\n",
    "\n",
    "parameter_count = num_qubits\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel_rx(theta: list[float]):\n",
    "    qubits = cudaq.qvector(num_qubits)\n",
    "\n",
    "    for i in range(num_qubits):\n",
    "        rx(theta[i], qubits[i])\n",
    "\n",
    "# Below we run a circuit for 500 different input parameters.\n",
    "parameters = np.random.default_rng(15).uniform(\n",
    "    low=0, high=1, size=(sample_count, parameter_count)\n",
    ")\n",
    "\n",
    "# Multi-GPU\n",
    "\n",
    "# We split our parameters into `num_qpus` arrays since we have `num_qpus` GPUs available.\n",
    "xi = np.split(parameters, num_qpus)\n",
    "\n",
    "print(\"We have\", parameter_count, \"parameters which we would like to execute\")\n",
    "\n",
    "print(\n",
    "    \"We split this into\",\n",
    "    len(xi),\n",
    "    \"batches of\",\n",
    "    \", \".join(str(xi[i].shape[0]) for i in range(num_qpus))\n",
    ")\n",
    "\n",
    "print(\"Shape after splitting\", xi[0].shape)\n",
    "\n",
    "\n",
    "asyncresults = []\n",
    "for i in range(num_qpus):\n",
    "    for j in range(xi[i].shape[0]):\n",
    "        qpu_id = i * 4 // len(xi)\n",
    "        asyncresults.append(\n",
    "            cudaq.observe_async(kernel_rx, ham, xi[i][j, :], qpu_id=qpu_id)\n",
    "        )\n",
    "\n",
    "print(\"Energies from multi-GPUs\")\n",
    "for result in asyncresults:\n",
    "    observe_result = result.get()\n",
    "    expectatoin = observe_result.expectation()\n",
    "#    print(expectatoin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch the Spin Hamiltonian terms:\n",
    "\n",
    "![img](./ham-batch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time (s) for single-GPU:  2.302815204486251\n",
      "Elapsed time (s) for multi-GPU:  4.726499784737825\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "import cudaq\n",
    "from cudaq import spin\n",
    "\n",
    "cudaq.set_target(\"nvidia-mqpu\")\n",
    "\n",
    "# cudaq.mpi.initialize()\n",
    "\n",
    "qubit_count = 22\n",
    "term_count = 100000\n",
    "\n",
    "\n",
    "@cudaq.kernel\n",
    "def batch_ham():\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    h(qubits[0])\n",
    "    for i in range(1, qubit_count):\n",
    "        x.ctrl(qubits[0], qubits[i])\n",
    "\n",
    "\n",
    "# We create a random Hamiltonian\n",
    "hamiltonian = cudaq.SpinOperator.random(qubit_count, term_count)\n",
    "\n",
    "# The observe calls allows us to calculate the expectation value of the Hamiltonian with respect to a specified kernel.\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "# Single node, single GPU.\n",
    "result = cudaq.observe(batch_ham, hamiltonian).expectation()\n",
    "end_time = timeit.default_timer()\n",
    "print(\"Elapsed time (s) for single-GPU: \", end_time - start_time)\n",
    "\n",
    "# If we have multiple GPUs/ QPUs available, we can parallelize the workflow with the addition of an argument in the observe call.\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "# Single node, multi-GPU.\n",
    "result = cudaq.observe(\n",
    "    batch_ham, hamiltonian, execution=cudaq.parallel.thread\n",
    ").expectation()\n",
    "end_time = timeit.default_timer()\n",
    "print(\"Elapsed time (s) for multi-GPU: \", end_time - start_time)\n",
    "\n",
    "\n",
    "# Multi-node, multi-GPU. (if included use mpirun -np n filename.py)\n",
    "# result = cudaq.observe(batch_ham, hamiltonian, execution=cudaq.parallel.mpi).expectation()\n",
    "\n",
    "# cudaq.mpi.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU (nvidia-mgpu)\n",
    "\n",
    "The `nvidia-mgpu` backend is useful for running a large single quantum circuit spread across multiple GPUs.\n",
    "- A $n$ qubit quantum state has $2^n$ complex amplitudes, each of which require 8 bytes of memory to store. Hence the total memory required to store a n qubit quantum state is $8$ bytes $\\times 2^n$. For $n=30$ qubits, this is roughly $8$ GB but for $n=40$, this exponentially increases to $8700$ GB.\n",
    "\n",
    "#### Example: GHZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# mpirun -np 4 python <fname> --target nvidia-mgpu\n",
    "\n",
    "import cudaq\n",
    "\n",
    "cudaq.mpi.initialize()\n",
    "\n",
    "qubit_count = 30\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel(qubit_num: int):\n",
    "    # Allocate our qubits.\n",
    "    qvector = cudaq.qvector(qubit_num)\n",
    "    # Place the first qubit in the superposition state.\n",
    "    h(qvector[0])\n",
    "    # Loop through the allocated qubits and apply controlled-X,\n",
    "    # or CNOT, operations between them.\n",
    "    for qubit in range(qubit_num - 1):\n",
    "        x.ctrl(qvector[qubit], qvector[qubit + 1])\n",
    "    # Measure the qubits.\n",
    "    mz(qvector)\n",
    "\n",
    "#print(\"Preparing GHZ state for\", qubit_count, \"qubits.\")\n",
    "counts = cudaq.sample(kernel, qubit_count)\n",
    "\n",
    "if cudaq.mpi.rank() == 0:\n",
    "    print(counts)\n",
    "\n",
    "cudaq.mpi.finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 111111111111111111111111111111:498 000000000000000000000000000000:502 }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ghz.py --target nvidia-mgpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
