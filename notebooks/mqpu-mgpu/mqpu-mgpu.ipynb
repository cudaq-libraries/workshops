{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-QPU (nvidia-mqpu)\n",
    "\n",
    "The `nvidia-mqpu` target is useful for distributing separate quantum circuits to individual GPUs on a single host machine. \n",
    "\n",
    "![mqpu](./circuit-mqpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with `sample` algorithmic primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of QPUs: 4\n",
      "qpu_id 0\n",
      "qpu_id 1\n",
      "qpu_id 2\n",
      "qpu_id 3\n",
      "qpu_id 0\n",
      "qpu_id 1\n",
      "qpu_id 2\n",
      "qpu_id 3\n",
      "qpu_id 0\n",
      "qpu_id 1\n",
      "{ 000:10000 }\n",
      "\n",
      "{ 000:9928 010:16 100:23 001:33 }\n",
      "\n",
      "{ 000:9713 010:98 100:80 110:2 001:103 101:3 011:1 }\n",
      "\n",
      "{ 000:9385 010:195 100:192 110:7 001:206 101:7 011:8 }\n",
      "\n",
      "{ 000:8796 010:372 111:1 100:389 110:13 001:396 101:20 011:13 }\n",
      "\n",
      "{ 000:8205 010:573 111:2 100:544 110:44 001:558 101:37 011:37 }\n",
      "\n",
      "{ 000:7597 010:747 111:8 100:734 110:77 001:702 101:71 011:64 }\n",
      "\n",
      "{ 000:6903 010:937 111:21 100:873 110:115 001:910 101:125 011:116 }\n",
      "\n",
      "{ 000:6130 010:1087 111:30 100:1059 110:209 001:1107 101:190 011:188 }\n",
      "\n",
      "{ 000:5369 010:1191 111:62 100:1238 110:287 001:1269 101:299 011:285 }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "\n",
    "cudaq.set_target(\"nvidia\", option=\"mqpu\")\n",
    "\n",
    "target = cudaq.get_target()\n",
    "num_qpus = target.num_qpus()\n",
    "print(\"Number of QPUs:\", num_qpus)\n",
    "\n",
    "\n",
    "@cudaq.kernel\n",
    "def test(i:int):\n",
    "    num_qubits = 3\n",
    "    qubits = cudaq.qvector(num_qubits)\n",
    "    ry(0.1 * i, qubits)\n",
    "    mz(qubits)\n",
    "\n",
    "\n",
    "count_futures = []\n",
    "for i in range(10):\n",
    "    qpu_id = i % num_qpus\n",
    "    print(\"qpu_id\", qpu_id)\n",
    "    result = cudaq.sample_async(test, i, shots_count=10000, qpu_id=qpu_id)\n",
    "    count_futures.append(result)\n",
    "\n",
    "for future in count_futures:\n",
    "    counts = future.get()\n",
    "    print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with `observe` algorithmic primitives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of QPUs: 4\n",
      "We have 10 parameters which we would like to execute\n",
      "We split this into 4 batches of 160, 160, 160, 160\n",
      "Shape after splitting (160, 10)\n",
      "Energies from multi-GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import cudaq\n",
    "from cudaq import spin\n",
    "\n",
    "cudaq.set_target(\"nvidia\", option=\"mqpu\")\n",
    "target = cudaq.get_target()\n",
    "num_qpus = target.num_qpus()\n",
    "print(\"Number of QPUs:\", num_qpus)\n",
    "\n",
    "num_qubits = 10\n",
    "sample_count = 640\n",
    "\n",
    "ham = spin.z(0)\n",
    "\n",
    "parameter_count = num_qubits\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel_rx(theta: list[float]):\n",
    "    qubits = cudaq.qvector(num_qubits)\n",
    "\n",
    "    for i in range(num_qubits):\n",
    "        rx(theta[i], qubits[i])\n",
    "\n",
    "# Below we run a circuit for 500 different input parameters.\n",
    "parameters = np.random.default_rng(15).uniform(\n",
    "    low=0, high=1, size=(sample_count, parameter_count)\n",
    ")\n",
    "\n",
    "# Multi-GPU\n",
    "# We split our parameters into `num_qpus` arrays since we have `num_qpus` GPUs available.\n",
    "xi = np.split(parameters, num_qpus)\n",
    "\n",
    "print(\"We have\", parameter_count, \"parameters which we would like to execute\")\n",
    "\n",
    "print(\n",
    "    \"We split this into\",\n",
    "    len(xi),\n",
    "    \"batches of\",\n",
    "    \", \".join(str(xi[i].shape[0]) for i in range(num_qpus))\n",
    ")\n",
    "\n",
    "print(\"Shape after splitting\", xi[0].shape)\n",
    "\n",
    "\n",
    "asyncresults = []\n",
    "for qpu_id in range(num_qpus):\n",
    "    for i in range(xi[qpu_id].shape[0]):\n",
    "        result = cudaq.observe_async(kernel_rx, ham, xi[qpu_id][i, :], qpu_id=qpu_id)\n",
    "        asyncresults.append(result)\n",
    "\n",
    "print(\"Energies from multi-GPUs\")\n",
    "for result in asyncresults:\n",
    "    observe_result = result.get()\n",
    "    expectation = observe_result.expectation()\n",
    "#    print(expectation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch the Spin Hamiltonian terms:\n",
    "\n",
    "<img src=\"./ham-batch.png\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time (s) for single-GPU:  2.703693811025005\n",
      "Elapsed time (s) for multi-GPU:  1.426821483997628\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "import cudaq\n",
    "from cudaq import spin\n",
    "\n",
    "cudaq.set_target(\"nvidia\", option=\"mqpu\")\n",
    "\n",
    "\n",
    "qubit_count = 22\n",
    "term_count = 100000\n",
    "\n",
    "\n",
    "@cudaq.kernel\n",
    "def batch_ham():\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    h(qubits[0])\n",
    "    for i in range(1, qubit_count):\n",
    "        x.ctrl(qubits[0], qubits[i])\n",
    "\n",
    "\n",
    "# We create a random Hamiltonian\n",
    "hamiltonian = cudaq.SpinOperator.random(qubit_count, term_count)\n",
    "\n",
    "# The observe calls allows us to calculate the expectation value of the Hamiltonian with respect to a specified kernel.\n",
    "start_time = timeit.default_timer()\n",
    "# Single node, single GPU.\n",
    "result = cudaq.observe(batch_ham, hamiltonian).expectation()\n",
    "end_time = timeit.default_timer()\n",
    "print(\"Elapsed time (s) for single-GPU: \", end_time - start_time)\n",
    "\n",
    "# If we have multiple GPUs/ QPUs available, we can parallelize the workflow with the addition of an argument in the observe call.\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "# Single node, multi-GPU.\n",
    "result = cudaq.observe(\n",
    "    batch_ham, hamiltonian, execution=cudaq.parallel.thread\n",
    ").expectation()\n",
    "end_time = timeit.default_timer()\n",
    "print(\"Elapsed time (s) for multi-GPU: \", end_time - start_time)\n",
    "\n",
    "\n",
    "# Multi-node, multi-GPU. (if included use mpirun -np n filename.py)\n",
    "# cudaq.mpi.initialize()\n",
    "# start_time = timeit.default_timer()\n",
    "# result = cudaq.observe(batch_ham, hamiltonian, execution=cudaq.parallel.mpi).expectation()\n",
    "# end_time = timeit.default_timer()\n",
    "# print(\"Elapsed time (s) for multi-GPU with mpi: \", end_time - start_time)\n",
    "# cudaq.mpi.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU (nvidia-mgpu)\n",
    "\n",
    "The `nvidia-mgpu` backend is useful for running a large single quantum circuit spread across multiple GPUs.\n",
    "- A $n$ qubit quantum state has $2^n$ complex amplitudes, each of which require 8 bytes of memory to store. Hence the total memory required to store a n qubit quantum state is $8$ bytes $\\times 2^n$. For $n=30$ qubits, this is roughly $8$ GB but for $n=40$, this exponentially increases to $8700$ GB.\n",
    "\n",
    "#### Example: GHZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# mpirun -np 4 python <fname> --target nvidia --target-option mgpu\n",
    "\n",
    "import cudaq\n",
    "\n",
    "cudaq.mpi.initialize()\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel(qubit_count: int):\n",
    "    # Allocate our qubits.\n",
    "    qvector = cudaq.qvector(qubit_count)\n",
    "    # Place the first qubit in the superposition state.\n",
    "    h(qvector[0])\n",
    "    # Loop through the allocated qubits and apply controlled-X,\n",
    "    # or CNOT, operations between them.\n",
    "    for qubit in range(qubit_count - 1):\n",
    "        x.ctrl(qvector[qubit], qvector[qubit + 1])\n",
    "    # Measure the qubits.\n",
    "    mz(qvector)\n",
    "\n",
    "#print(\"Preparing GHZ state for\", qubit_count, \"qubits.\")\n",
    "qubit_count = 30\n",
    "counts = cudaq.sample(kernel, qubit_count)\n",
    "\n",
    "if cudaq.mpi.rank() == 0:\n",
    "    print(counts)\n",
    "\n",
    "cudaq.mpi.finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 000000000000000000000000000000:513 111111111111111111111111111111:487 }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ghz.py --target nvidia --target-option mgpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch job\n",
    "\n",
    "Prepare the script file like `script.sh`:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#$ -l rt_F=1\n",
    "#$ -l h_rt=0:00:10\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "source /etc/profile.d/modules.sh\n",
    "module load singularitypro\n",
    "\n",
    "singularity exec --nv cuda-quantum_0.8.0.sif python ghz.py --target nvidia --target-option mgpu,fp32\n",
    "```\n",
    "\n",
    "Get container and run the script:\n",
    "```sh\n",
    "SINGULARITY_TMPDIR=$SGE_LOCALDIR singularity pull docker://nvcr.io/nvidia/quantum/cuda-quantum:0.8.0\n",
    "qsub -g grpname script.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "\n",
    "## Exercise\n",
    "\n",
    "What is the maximum number of qubits that can be simulated by one GPU?\n",
    "What is the maximum number of qubits that can be simulated by one node?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
